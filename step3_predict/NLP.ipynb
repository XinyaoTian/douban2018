{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/bigdata/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aka</th>\n",
       "      <th>card_subtitle</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>countries</th>\n",
       "      <th>cover.author.avatar</th>\n",
       "      <th>cover.author.id</th>\n",
       "      <th>cover.author.loc.name</th>\n",
       "      <th>cover.author.loc.uid</th>\n",
       "      <th>cover.author.name</th>\n",
       "      <th>cover.author.uid</th>\n",
       "      <th>...</th>\n",
       "      <th>trailer.create_time</th>\n",
       "      <th>trailer.n_comments</th>\n",
       "      <th>trailer.runtime</th>\n",
       "      <th>year</th>\n",
       "      <th>actor_id</th>\n",
       "      <th>actor</th>\n",
       "      <th>actor_roles</th>\n",
       "      <th>director_id</th>\n",
       "      <th>director</th>\n",
       "      <th>director_roles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>[心跳纽约(港), 纽约寂寞男孩(台)]</td>\n",
       "      <td>2017 / 美国 / 剧情 / 马克·韦布 / 卡勒姆·特纳 杰夫·布里吉斯</td>\n",
       "      <td>396</td>\n",
       "      <td>[美国]</td>\n",
       "      <td>https://img3.doubanio.com/icon/up139015254-13.jpg</td>\n",
       "      <td>139015254</td>\n",
       "      <td>洛阳</td>\n",
       "      <td>luoyang</td>\n",
       "      <td>横道世之介</td>\n",
       "      <td>139015254</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>02:25</td>\n",
       "      <td>2017</td>\n",
       "      <td>1328007</td>\n",
       "      <td>卡勒姆·特纳</td>\n",
       "      <td>[演员]</td>\n",
       "      <td>1049688</td>\n",
       "      <td>马克·韦布</td>\n",
       "      <td>[导演, 制片, 编剧]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[淑女鸟(台), 不得鸟小姐(港), 大鸟萌妹(豆友译名), 伯德夫人, 柏德小姐, 小仙鸟...</td>\n",
       "      <td>2017 / 美国 / 剧情 喜剧 家庭 / 格蕾塔·葛韦格 / 西尔莎·罗南 劳里·梅特卡夫</td>\n",
       "      <td>23830</td>\n",
       "      <td>[美国]</td>\n",
       "      <td>https://img3.doubanio.com/icon/up52071601-3.jpg</td>\n",
       "      <td>52071601</td>\n",
       "      <td>武汉</td>\n",
       "      <td>wuhan</td>\n",
       "      <td>Torrilla</td>\n",
       "      <td>52071601</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>31.0</td>\n",
       "      <td>02:29</td>\n",
       "      <td>2017</td>\n",
       "      <td>1022004</td>\n",
       "      <td>西尔莎·罗南</td>\n",
       "      <td>[演员]</td>\n",
       "      <td>1022652</td>\n",
       "      <td>格蕾塔·葛韦格</td>\n",
       "      <td>[演员, 编剧, 配音, 导演, 制片]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[宠物小精灵：就决定是你了, 神奇宝贝：就决定是你了, Pokémon the Movie:...</td>\n",
       "      <td>2017 / 日本 / 动画 奇幻 冒险 / 汤山邦彦 / 松本梨香 大谷育江</td>\n",
       "      <td>1025</td>\n",
       "      <td>[日本]</td>\n",
       "      <td>https://img3.doubanio.com/icon/up3273352-362.jpg</td>\n",
       "      <td>3273352</td>\n",
       "      <td>广州</td>\n",
       "      <td>guangzhou</td>\n",
       "      <td>朗</td>\n",
       "      <td>ChanKingLong</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>01:32</td>\n",
       "      <td>2017</td>\n",
       "      <td>1040476</td>\n",
       "      <td>松本梨香</td>\n",
       "      <td>[演员, 导演]</td>\n",
       "      <td>1029060</td>\n",
       "      <td>汤山邦彦</td>\n",
       "      <td>[导演, 编剧]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[阴阳路之常在你左右, Accompany You, Always Be with You]</td>\n",
       "      <td>2017 / 香港 中国大陆 / 犯罪 悬疑 / 邱礼涛 / 古天乐 张智霖</td>\n",
       "      <td>2983</td>\n",
       "      <td>[香港, 中国大陆]</td>\n",
       "      <td>https://img3.doubanio.com/icon/up146443262-4.jpg</td>\n",
       "      <td>146443262</td>\n",
       "      <td>北京</td>\n",
       "      <td>beijing</td>\n",
       "      <td>你看看</td>\n",
       "      <td>146443262</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-09-22</td>\n",
       "      <td>26.0</td>\n",
       "      <td>01:32</td>\n",
       "      <td>2017</td>\n",
       "      <td>1027577</td>\n",
       "      <td>古天乐</td>\n",
       "      <td>[演员, 编剧, 制片]</td>\n",
       "      <td>1274313</td>\n",
       "      <td>邱礼涛</td>\n",
       "      <td>[导演, 摄影, 编剧, 演员, 制片]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[我想吃了你的胰脏, I Want to Eat Your Pancreas]</td>\n",
       "      <td>2017 / 日本 / 剧情 爱情 / 月川翔 / 滨边美波 北村匠海</td>\n",
       "      <td>1624</td>\n",
       "      <td>[日本]</td>\n",
       "      <td>https://img3.doubanio.com/icon/up53294996-4.jpg</td>\n",
       "      <td>53294996</td>\n",
       "      <td>南京</td>\n",
       "      <td>nanjing</td>\n",
       "      <td>红色鲱鱼</td>\n",
       "      <td>53294996</td>\n",
       "      <td>...</td>\n",
       "      <td>2017-09-16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>01:32</td>\n",
       "      <td>2017</td>\n",
       "      <td>1324483</td>\n",
       "      <td>滨边美波</td>\n",
       "      <td>[演员]</td>\n",
       "      <td>1372453</td>\n",
       "      <td>月川翔</td>\n",
       "      <td>[导演, 编剧, 剪辑]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   aka  \\\n",
       "114                               [心跳纽约(港), 纽约寂寞男孩(台)]   \n",
       "63   [淑女鸟(台), 不得鸟小姐(港), 大鸟萌妹(豆友译名), 伯德夫人, 柏德小姐, 小仙鸟...   \n",
       "22   [宠物小精灵：就决定是你了, 神奇宝贝：就决定是你了, Pokémon the Movie:...   \n",
       "120     [阴阳路之常在你左右, Accompany You, Always Be with You]   \n",
       "93             [我想吃了你的胰脏, I Want to Eat Your Pancreas]   \n",
       "\n",
       "                                       card_subtitle  comment_count  \\\n",
       "114          2017 / 美国 / 剧情 / 马克·韦布 / 卡勒姆·特纳 杰夫·布里吉斯            396   \n",
       "63   2017 / 美国 / 剧情 喜剧 家庭 / 格蕾塔·葛韦格 / 西尔莎·罗南 劳里·梅特卡夫          23830   \n",
       "22           2017 / 日本 / 动画 奇幻 冒险 / 汤山邦彦 / 松本梨香 大谷育江           1025   \n",
       "120           2017 / 香港 中国大陆 / 犯罪 悬疑 / 邱礼涛 / 古天乐 张智霖           2983   \n",
       "93               2017 / 日本 / 剧情 爱情 / 月川翔 / 滨边美波 北村匠海           1624   \n",
       "\n",
       "      countries                                cover.author.avatar  \\\n",
       "114        [美国]  https://img3.doubanio.com/icon/up139015254-13.jpg   \n",
       "63         [美国]    https://img3.doubanio.com/icon/up52071601-3.jpg   \n",
       "22         [日本]   https://img3.doubanio.com/icon/up3273352-362.jpg   \n",
       "120  [香港, 中国大陆]   https://img3.doubanio.com/icon/up146443262-4.jpg   \n",
       "93         [日本]    https://img3.doubanio.com/icon/up53294996-4.jpg   \n",
       "\n",
       "     cover.author.id cover.author.loc.name cover.author.loc.uid  \\\n",
       "114        139015254                    洛阳              luoyang   \n",
       "63          52071601                    武汉                wuhan   \n",
       "22           3273352                    广州            guangzhou   \n",
       "120        146443262                    北京              beijing   \n",
       "93          53294996                    南京              nanjing   \n",
       "\n",
       "    cover.author.name cover.author.uid          ...           \\\n",
       "114             横道世之介        139015254          ...            \n",
       "63           Torrilla         52071601          ...            \n",
       "22                  朗     ChanKingLong          ...            \n",
       "120               你看看        146443262          ...            \n",
       "93               红色鲱鱼         53294996          ...            \n",
       "\n",
       "    trailer.create_time  trailer.n_comments trailer.runtime  year actor_id  \\\n",
       "114          2017-08-17                 0.0           02:25  2017  1328007   \n",
       "63           2017-12-06                31.0           02:29  2017  1022004   \n",
       "22           2017-06-14                 5.0           01:32  2017  1040476   \n",
       "120          2017-09-22                26.0           01:32  2017  1027577   \n",
       "93           2017-09-16                 4.0           01:32  2017  1324483   \n",
       "\n",
       "      actor   actor_roles director_id director        director_roles  \n",
       "114  卡勒姆·特纳          [演员]     1049688    马克·韦布          [导演, 制片, 编剧]  \n",
       "63   西尔莎·罗南          [演员]     1022652  格蕾塔·葛韦格  [演员, 编剧, 配音, 导演, 制片]  \n",
       "22     松本梨香      [演员, 导演]     1029060     汤山邦彦              [导演, 编剧]  \n",
       "120     古天乐  [演员, 编剧, 制片]     1274313      邱礼涛  [导演, 摄影, 编剧, 演员, 制片]  \n",
       "93     滨边美波          [演员]     1372453      月川翔          [导演, 编剧, 剪辑]  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "movies_df = pd.read_json('movie300new_mobile_info.json')\n",
    "choosed_columns = ['actors',        #参演人员，所有的演员的信息    ==待处理==\n",
    "                   'aka',           #别名\n",
    "                   'card_subtitle', #日期，国家，类型，主演....\n",
    "                   'comment_count', #评论数量\n",
    "                   'countries',     #国家 ， 好像card_subtitle里面有，暂时保留\n",
    "                   'cover.author.avatar', #编辑这个的编辑的头像\n",
    "                   'cover.author.id', #编辑这个电影的编辑的id\n",
    "                   'cover.author.loc.name', #编辑这个电影的编辑所在地区\n",
    "                   'cover.author.loc.uid',#编辑这个电影的编辑所在地区的id（拼音）\n",
    "                   'cover.author.name', ##编辑这个电影的编辑的姓名\n",
    "                   'cover.author.uid', #编辑这个电影的编辑的id\n",
    "                   'cover.author.url', #编辑这个电影的编辑的个人主页\n",
    "                   'cover.comments_count', #编辑这个电影的Cover的评论数\n",
    "                   'cover.create_time',    #编辑这个电影的Cover创建时间\n",
    "                   'directors',      #导演信息    ==待处理==\n",
    "                   'durations',      #电影的时长\n",
    "                   'genres',         #电影的类型\n",
    "                   'has_linewatch',  #有没有在线观看的链接\n",
    "                   'id',             #影片的id，鉴别不同的影片\n",
    "                   'info_url',       #url链接里面包含了：片名，原名，又名，导演，主演，上映，类型，片场，地区，语言，IMDB的链接\n",
    "                   'intro',          #电影简介\n",
    "                   'is_douban_intro', #是不是豆瓣的简介，还是其它地方的\n",
    "                   'languages',      #语言类型\n",
    "                   'lineticket_url', #可以在线购买的链接（和猫眼合作的）\n",
    "                   'original_title', #原名，里面有部分是空值\n",
    "                   'pubdate',        #公映时间和发布地方\n",
    "                   'rating.count',   #打分的数量\n",
    "                   'rating.star_count', #以5星为满星的 分值\n",
    "                   'rating.value',   #以10分为满分的 分值   ===？？和上一个不同，是不是有专业用户和一般用户的综合？或者中和imdb？？？\n",
    "                   'release_date',   #和前面的公映时间pubdate一样，少了发布的地方\n",
    "                   'review_count',   #暂时不知道是什么意思，留着\n",
    "                   'title',          #影片名\n",
    "                   'trailer.create_time', #试看的时间，应该可以和pubtime做一个对比，看一般多久会出试看视频\n",
    "                   'trailer.n_comments', #预告片的评论数，感觉应该和影片评论数正相关\n",
    "                   'trailer.runtime',    #预告片的时长\n",
    "                   'year'            #年，==？？暂时不知道什么时间\n",
    "                  ]\n",
    "movie300_choosed_column = movies_df[choosed_columns]\n",
    "#actor中，准备只提取一号演员的id姓名和角色，主演中的主演\n",
    "#写一个函数:\n",
    "def get_actors_info(actor):\n",
    "    try:\n",
    "        actor_id = actor[0]['id']\n",
    "        actor_name = actor[0]['name']\n",
    "        actor_roles = actor[0]['roles']\n",
    "    except:\n",
    "        actor_id = ''\n",
    "        actor_name = ''\n",
    "        actor_roles = ''\n",
    "    return actor_id,actor_name,actor_roles\n",
    "movie300_choosed_column['actor_id'],movie300_choosed_column['actor'],movie300_choosed_column['actor_roles'] = zip(*list(movie300_choosed_column['actors'].apply(lambda x:get_actors_info(x))))\n",
    "#下面写个函数，提出导演的信息，发现可以用和演员一样的玩儿法\n",
    "movie300_choosed_column['director_id'],movie300_choosed_column['director'],movie300_choosed_column['director_roles'] = zip(*list(movie300_choosed_column['directors'].apply(lambda x:get_actors_info(x))))\n",
    "drop_list = ['directors','actors']\n",
    "movie300_choosed_column = movie300_choosed_column.drop(drop_list,axis=1)\n",
    "movie300_choosed_column.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>create_time</th>\n",
       "      <th>id</th>\n",
       "      <th>index</th>\n",
       "      <th>rating.value</th>\n",
       "      <th>sharing_url</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>user.avatar</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.loc.name</th>\n",
       "      <th>user.loc.uid</th>\n",
       "      <th>user.name</th>\n",
       "      <th>user.uid</th>\n",
       "      <th>user.url</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81334</th>\n",
       "      <td>咋欧洲人也有碟仙一说？看来这种灵异游戏不是亚洲的专利啊！</td>\n",
       "      <td>2018-01-30 23:35:49</td>\n",
       "      <td>1315833480</td>\n",
       "      <td>166</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://www.douban.com/doubanapp/dispatch?uri=...</td>\n",
       "      <td>灵蚀</td>\n",
       "      <td>https://movie.douban.com/subject/27126455/</td>\n",
       "      <td>https://img3.doubanio.com/icon/u97839460-1.jpg</td>\n",
       "      <td>97839460</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>铃兰的冬天</td>\n",
       "      <td>97839460</td>\n",
       "      <td>https://www.douban.com/people/97839460/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23410</th>\n",
       "      <td>剧本真的烂 很多矛盾的地方</td>\n",
       "      <td>2017-11-14 13:02:58</td>\n",
       "      <td>1271020215</td>\n",
       "      <td>492</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://www.douban.com/doubanapp/dispatch?uri=...</td>\n",
       "      <td>24小时：末路重生</td>\n",
       "      <td>https://movie.douban.com/subject/26724807/</td>\n",
       "      <td>https://img3.doubanio.com/icon/up163710765-2.jpg</td>\n",
       "      <td>163710765</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>在在在在看你</td>\n",
       "      <td>163710765</td>\n",
       "      <td>https://www.douban.com/people/163710765/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73961</th>\n",
       "      <td>李钟硕太烂了，他这个角色的呈现让吴亦凡上都可以搞定。我最喜欢金明民，以及帮助他开了两次逮捕令...</td>\n",
       "      <td>2017-09-30 01:26:55</td>\n",
       "      <td>1188644661</td>\n",
       "      <td>70</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.douban.com/doubanapp/dispatch?uri=...</td>\n",
       "      <td>杀人优越权</td>\n",
       "      <td>https://movie.douban.com/subject/26858970/</td>\n",
       "      <td>https://img3.doubanio.com/icon/up3594852-12.jpg</td>\n",
       "      <td>3594852</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>葛团妹</td>\n",
       "      <td>3594852</td>\n",
       "      <td>https://www.douban.com/people/3594852/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57711</th>\n",
       "      <td>虽然情节单薄低幼，但是那个微信气泡狗真是把我萌翻了</td>\n",
       "      <td>2017-11-06 00:26:58</td>\n",
       "      <td>1267204970</td>\n",
       "      <td>160</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://www.douban.com/doubanapp/dispatch?uri=...</td>\n",
       "      <td>表情奇幻冒险</td>\n",
       "      <td>https://movie.douban.com/subject/26577354/</td>\n",
       "      <td>https://img3.doubanio.com/icon/up2430206-15.jpg</td>\n",
       "      <td>2430206</td>\n",
       "      <td>北京</td>\n",
       "      <td>beijing</td>\n",
       "      <td>Fujihime Arashi</td>\n",
       "      <td>fengzaifei</td>\n",
       "      <td>https://www.douban.com/people/2430206/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62594</th>\n",
       "      <td>一部悬疑片引发出对女权的思考，描述了当时英国大环境下的女性地位。很惊喜</td>\n",
       "      <td>2017-10-25 04:24:57</td>\n",
       "      <td>1261617554</td>\n",
       "      <td>456</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.douban.com/doubanapp/dispatch?uri=...</td>\n",
       "      <td>莱姆豪斯的杀人魔</td>\n",
       "      <td>https://movie.douban.com/subject/26616731/</td>\n",
       "      <td>https://img3.doubanio.com/icon/up53556694-5.jpg</td>\n",
       "      <td>53556694</td>\n",
       "      <td>南京</td>\n",
       "      <td>nanjing</td>\n",
       "      <td>胶原蛋白小天使</td>\n",
       "      <td>53556694</td>\n",
       "      <td>https://www.douban.com/people/53556694/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment         create_time  \\\n",
       "81334                       咋欧洲人也有碟仙一说？看来这种灵异游戏不是亚洲的专利啊！ 2018-01-30 23:35:49   \n",
       "23410                                      剧本真的烂 很多矛盾的地方 2017-11-14 13:02:58   \n",
       "73961  李钟硕太烂了，他这个角色的呈现让吴亦凡上都可以搞定。我最喜欢金明民，以及帮助他开了两次逮捕令... 2017-09-30 01:26:55   \n",
       "57711                          虽然情节单薄低幼，但是那个微信气泡狗真是把我萌翻了 2017-11-06 00:26:58   \n",
       "62594                一部悬疑片引发出对女权的思考，描述了当时英国大环境下的女性地位。很惊喜 2017-10-25 04:24:57   \n",
       "\n",
       "               id  index  rating.value  \\\n",
       "81334  1315833480    166           3.0   \n",
       "23410  1271020215    492           3.0   \n",
       "73961  1188644661     70           4.0   \n",
       "57711  1267204970    160           3.0   \n",
       "62594  1261617554    456           4.0   \n",
       "\n",
       "                                             sharing_url      title  \\\n",
       "81334  https://www.douban.com/doubanapp/dispatch?uri=...         灵蚀   \n",
       "23410  https://www.douban.com/doubanapp/dispatch?uri=...  24小时：末路重生   \n",
       "73961  https://www.douban.com/doubanapp/dispatch?uri=...      杀人优越权   \n",
       "57711  https://www.douban.com/doubanapp/dispatch?uri=...     表情奇幻冒险   \n",
       "62594  https://www.douban.com/doubanapp/dispatch?uri=...   莱姆豪斯的杀人魔   \n",
       "\n",
       "                                              url  \\\n",
       "81334  https://movie.douban.com/subject/27126455/   \n",
       "23410  https://movie.douban.com/subject/26724807/   \n",
       "73961  https://movie.douban.com/subject/26858970/   \n",
       "57711  https://movie.douban.com/subject/26577354/   \n",
       "62594  https://movie.douban.com/subject/26616731/   \n",
       "\n",
       "                                            user.avatar    user.id  \\\n",
       "81334    https://img3.doubanio.com/icon/u97839460-1.jpg   97839460   \n",
       "23410  https://img3.doubanio.com/icon/up163710765-2.jpg  163710765   \n",
       "73961   https://img3.doubanio.com/icon/up3594852-12.jpg    3594852   \n",
       "57711   https://img3.doubanio.com/icon/up2430206-15.jpg    2430206   \n",
       "62594   https://img3.doubanio.com/icon/up53556694-5.jpg   53556694   \n",
       "\n",
       "      user.loc.name user.loc.uid        user.name    user.uid  \\\n",
       "81334          None         None            铃兰的冬天    97839460   \n",
       "23410          None         None           在在在在看你   163710765   \n",
       "73961          None         None              葛团妹     3594852   \n",
       "57711            北京      beijing  Fujihime Arashi  fengzaifei   \n",
       "62594            南京      nanjing          胶原蛋白小天使    53556694   \n",
       "\n",
       "                                       user.url  vote_count  \n",
       "81334   https://www.douban.com/people/97839460/           0  \n",
       "23410  https://www.douban.com/people/163710765/           0  \n",
       "73961    https://www.douban.com/people/3594852/           0  \n",
       "57711    https://www.douban.com/people/2430206/           0  \n",
       "62594   https://www.douban.com/people/53556694/           0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_json('structure_comments.json')\n",
    "comments.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#感觉电影中的现有评分对大部分人有影响，这个是属于羊群效应，所以把现有评分加入到评分做一个feature\n",
    "#先学会怎么做NLP再做这个工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = pd.merge(comments,movie300_choosed_column[['title','rating.value']],how='left',on='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 采用评论的内容和打分进行分析\n",
    "* 小于等于2是差评\n",
    "* 等于3是中评\n",
    "* 大于等于4是好评\n",
    "\n",
    "#### 重新修正为2分问题\n",
    "* 小于等于3是差评\n",
    "* 大于等于4是好评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npl_df = comments[['comment','title','rating.value_y','rating.value_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96596"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(npl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating.value_y</th>\n",
       "      <th>rating.value_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>96359.000000</td>\n",
       "      <td>93655.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.770185</td>\n",
       "      <td>3.350456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.942223</td>\n",
       "      <td>1.003897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.100000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.700000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating.value_y  rating.value_x\n",
       "count    96359.000000    93655.000000\n",
       "mean         6.770185        3.350456\n",
       "std          0.942223        1.003897\n",
       "min          5.000000        1.000000\n",
       "25%          6.100000        3.000000\n",
       "50%          6.700000        3.000000\n",
       "75%          7.400000        4.000000\n",
       "max          9.200000        5.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npl_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去掉npl_df中的nan值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npl_df = npl_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>title</th>\n",
       "      <th>rating.value_y</th>\n",
       "      <th>rating.value_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13751</th>\n",
       "      <td>首先，我是看到王敏奕主演才看的，然后电影还不错</td>\n",
       "      <td>初恋日记</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34791</th>\n",
       "      <td>最后结局解释了子弹的bug，但“过去的故事”还是无法与之前的相比，本作是又一波屠戮开始的前奏...</td>\n",
       "      <td>电锯惊魂8：竖锯</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34701</th>\n",
       "      <td>人类再次回想起被哥斯拉胖揍的恐怖，结尾好评不过也猜得到，如果只是无脑的打打打应该凑不了三部的...</td>\n",
       "      <td>哥斯拉：怪兽行星</td>\n",
       "      <td>6.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95405</th>\n",
       "      <td>二星为了这电影拍摄环境艰苦给的。</td>\n",
       "      <td>远山恋人</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68093</th>\n",
       "      <td>三流动作片 故事情节处理也是不行</td>\n",
       "      <td>赶尽杀绝</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79302</th>\n",
       "      <td>个人评分7.3分</td>\n",
       "      <td>你好布拉德</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40023</th>\n",
       "      <td>a poor masturbation but still orgasmic.</td>\n",
       "      <td>酒会</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88936</th>\n",
       "      <td>整体就是故作玄虚的做作 但看到最后竟然觉得有点惊喜 感觉导演想要在最后来个主题升华 而前面的...</td>\n",
       "      <td>方法派</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40999</th>\n",
       "      <td>剧情其实有一点勉强 但是这个颜值是让人无法拒绝啊 女主男主女二男二全都好看呐 翔平alan真...</td>\n",
       "      <td>昼行闪耀的流星</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56672</th>\n",
       "      <td>很细腻，对胃口。</td>\n",
       "      <td>时间中的孩子</td>\n",
       "      <td>6.6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment     title  \\\n",
       "13751                            首先，我是看到王敏奕主演才看的，然后电影还不错      初恋日记   \n",
       "34791  最后结局解释了子弹的bug，但“过去的故事”还是无法与之前的相比，本作是又一波屠戮开始的前奏...  电锯惊魂8：竖锯   \n",
       "34701  人类再次回想起被哥斯拉胖揍的恐怖，结尾好评不过也猜得到，如果只是无脑的打打打应该凑不了三部的...  哥斯拉：怪兽行星   \n",
       "95405                                   二星为了这电影拍摄环境艰苦给的。      远山恋人   \n",
       "68093                                   三流动作片 故事情节处理也是不行      赶尽杀绝   \n",
       "79302                                           个人评分7.3分     你好布拉德   \n",
       "40023            a poor masturbation but still orgasmic.        酒会   \n",
       "88936  整体就是故作玄虚的做作 但看到最后竟然觉得有点惊喜 感觉导演想要在最后来个主题升华 而前面的...       方法派   \n",
       "40999  剧情其实有一点勉强 但是这个颜值是让人无法拒绝啊 女主男主女二男二全都好看呐 翔平alan真...   昼行闪耀的流星   \n",
       "56672                                           很细腻，对胃口。    时间中的孩子   \n",
       "\n",
       "       rating.value_y  rating.value_x  \n",
       "13751             5.6             4.0  \n",
       "34791             6.9             2.0  \n",
       "34701             6.3             4.0  \n",
       "95405             6.0             2.0  \n",
       "68093             5.1             2.0  \n",
       "79302             7.5             3.0  \n",
       "40023             7.3             4.0  \n",
       "88936             6.6             2.0  \n",
       "40999             6.2             4.0  \n",
       "56672             6.6             4.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-24:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import numpy    #numpy计算包\n",
    "import jieba    #分词包\n",
    "import re\n",
    "#jieba.enable_parallel() # 关闭并行分词\n",
    "jieba.enable_parallel(8) # 开启并行分词模式，参数为并行进程数 \n",
    "\n",
    "content = npl_df[npl_df['title']=='寻梦环游记'].comment.values.tolist()\n",
    "segment = []\n",
    "for line in content:\n",
    "    try:\n",
    "        segs = jieba.lcut(line)\n",
    "        for seg in segs:\n",
    "            if len(seg)>1 and seg!='\\r\\n':\n",
    "                segment.append(seg)\n",
    "    except:\n",
    "        print(line)\n",
    "        continue\n",
    "        \n",
    "\n",
    "words_df=pd.DataFrame({'segment':segment})\n",
    "#停用词\n",
    "stopwords=pd.read_csv(\"stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')#quoting=3全不引用\n",
    "words_df=words_df[~words_df.segment.isin(stopwords.stopword)]\n",
    "#词频统计\n",
    "words_stat = words_df.groupby('segment')['segment'].agg({'count':'size'}).reset_index().sort_values(by=[\"count\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>皮克斯</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>故事</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>电影</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>世界</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>亡灵</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>一个</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>墨西哥</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>梦想</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>死亡</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3690</th>\n",
       "      <td>音乐</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     segment  count\n",
       "2796     皮克斯    112\n",
       "2100      故事     96\n",
       "2755      电影     96\n",
       "421       世界     94\n",
       "540       亡灵     78\n",
       "229       一个     75\n",
       "1266     墨西哥     69\n",
       "2330      梦想     61\n",
       "2384      死亡     60\n",
       "3690      音乐     60"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_stat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重要词抽取,给予tf-idf的抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('皮克斯', 0.14757117507336492),\n",
       " ('亡灵', 0.06879938469606824),\n",
       " ('电影', 0.06058355014230143),\n",
       " ('故事', 0.055692507881584835),\n",
       " ('动画', 0.05155355050208531),\n",
       " ('墨西哥', 0.05008846944738199),\n",
       " ('梦想', 0.044810859516145024),\n",
       " ('世界', 0.03891448001171374),\n",
       " ('音乐', 0.03770427931956398),\n",
       " ('套路', 0.03659595226513744)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "content = \"\".join(content)\n",
    "#print(\" \".join)\n",
    "analyse.extract_tags(content, topK=10, withWeight=True, allowPOS=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('故事', 1.0),\n",
       " ('电影', 0.8668683339873232),\n",
       " ('亡灵', 0.8497028314355037),\n",
       " ('墨西哥', 0.789898676471442),\n",
       " ('世界', 0.7005461790365071),\n",
       " ('音乐', 0.6173455852594754),\n",
       " ('家庭', 0.546327163573403),\n",
       " ('梦想', 0.523774878509023),\n",
       " ('动画', 0.5003970629730443),\n",
       " ('文化', 0.41724868281452165),\n",
       " ('记忆', 0.3382486600614563),\n",
       " ('亲情', 0.3241545739167724),\n",
       " ('套路', 0.2997329983698173),\n",
       " ('剧情', 0.28458186354115034),\n",
       " ('家人', 0.2574312451366974),\n",
       " ('时候', 0.24129007242782752),\n",
       " ('骷髅', 0.22693022700993812),\n",
       " ('情感', 0.2215232310676254),\n",
       " ('观众', 0.2197127353815787),\n",
       " ('细节', 0.19481810327737428)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "\n",
    "(analyse.textrank(content, topK=20, withWeight=True, allowPOS=('ns','n','vn')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主题模型(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '?', '、', ..., '上', '上来', '上去'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "stopwords=pd.read_csv(\"stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')#quoting=3全不引用\n",
    "stopwords.stopword.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def is_sqr(x):\n",
    "    return math.sqrt(x) % 1 == 0\n",
    " \n",
    "newlist = list(filter(is_sqr, range(1, 101)))\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = npl_df[(npl_df['title']=='寻梦环游记')][:20].comment.values.tolist()\n",
    "sentences = []\n",
    "for line in content:\n",
    "    try:\n",
    "        segs = jieba.lcut(line)\n",
    "        \n",
    "        segs = list(filter(lambda x:len(x)>1, segs))\n",
    "        segs = list(filter(lambda x:x not in stopwords.stopword.values, segs))\n",
    "        sentences.append(segs)\n",
    "    except:\n",
    "        print(line)\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1)],\n",
       " [(3, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1)],\n",
       " [(3, 1),\n",
       "  (58, 1),\n",
       "  (62, 1),\n",
       "  (63, 2),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1),\n",
       "  (71, 2),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 1),\n",
       "  (82, 1)],\n",
       " [(80, 1),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (87, 1),\n",
       "  (88, 1),\n",
       "  (89, 1),\n",
       "  (90, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 1),\n",
       "  (94, 1),\n",
       "  (95, 1),\n",
       "  (96, 1),\n",
       "  (97, 1)],\n",
       " [(98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 1),\n",
       "  (102, 1),\n",
       "  (103, 1),\n",
       "  (104, 1),\n",
       "  (105, 1),\n",
       "  (106, 1),\n",
       "  (107, 1),\n",
       "  (108, 1),\n",
       "  (109, 1),\n",
       "  (110, 1),\n",
       "  (111, 1),\n",
       "  (112, 1)],\n",
       " [(82, 1),\n",
       "  (85, 1),\n",
       "  (113, 2),\n",
       "  (114, 1),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 1),\n",
       "  (118, 1),\n",
       "  (119, 1),\n",
       "  (120, 1),\n",
       "  (121, 1),\n",
       "  (122, 1),\n",
       "  (123, 1),\n",
       "  (124, 1),\n",
       "  (125, 1),\n",
       "  (126, 1),\n",
       "  (127, 1),\n",
       "  (128, 1),\n",
       "  (129, 1),\n",
       "  (130, 1),\n",
       "  (131, 1),\n",
       "  (132, 1),\n",
       "  (133, 1),\n",
       "  (134, 1),\n",
       "  (135, 1),\n",
       "  (136, 1),\n",
       "  (137, 1),\n",
       "  (138, 1),\n",
       "  (139, 1),\n",
       "  (140, 1),\n",
       "  (141, 1)],\n",
       " [(82, 1), (142, 1), (143, 1), (144, 1), (145, 1)],\n",
       " [(3, 1),\n",
       "  (15, 1),\n",
       "  (54, 1),\n",
       "  (68, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (105, 1),\n",
       "  (138, 1),\n",
       "  (146, 1),\n",
       "  (147, 1),\n",
       "  (148, 1),\n",
       "  (149, 1),\n",
       "  (150, 1),\n",
       "  (151, 1),\n",
       "  (152, 1),\n",
       "  (153, 1),\n",
       "  (154, 1),\n",
       "  (155, 1),\n",
       "  (156, 1),\n",
       "  (157, 1),\n",
       "  (158, 1),\n",
       "  (159, 1),\n",
       "  (160, 1),\n",
       "  (161, 1),\n",
       "  (162, 1),\n",
       "  (163, 1),\n",
       "  (164, 1),\n",
       "  (165, 1),\n",
       "  (166, 1),\n",
       "  (167, 1),\n",
       "  (168, 1),\n",
       "  (169, 1),\n",
       "  (170, 1),\n",
       "  (171, 1),\n",
       "  (172, 1),\n",
       "  (173, 1),\n",
       "  (174, 1),\n",
       "  (175, 1),\n",
       "  (176, 1),\n",
       "  (177, 1),\n",
       "  (178, 1),\n",
       "  (179, 1),\n",
       "  (180, 1)],\n",
       " [(3, 1),\n",
       "  (17, 1),\n",
       "  (68, 1),\n",
       "  (79, 1),\n",
       "  (104, 1),\n",
       "  (138, 1),\n",
       "  (171, 1),\n",
       "  (181, 1),\n",
       "  (182, 1),\n",
       "  (183, 1),\n",
       "  (184, 1),\n",
       "  (185, 1),\n",
       "  (186, 1),\n",
       "  (187, 1),\n",
       "  (188, 1),\n",
       "  (189, 1),\n",
       "  (190, 1),\n",
       "  (191, 1),\n",
       "  (192, 1),\n",
       "  (193, 1),\n",
       "  (194, 1),\n",
       "  (195, 1),\n",
       "  (196, 1),\n",
       "  (197, 1),\n",
       "  (198, 1),\n",
       "  (199, 1),\n",
       "  (200, 1),\n",
       "  (201, 1),\n",
       "  (202, 1),\n",
       "  (203, 1),\n",
       "  (204, 1),\n",
       "  (205, 1),\n",
       "  (206, 1),\n",
       "  (207, 1),\n",
       "  (208, 1),\n",
       "  (209, 1),\n",
       "  (210, 1),\n",
       "  (211, 1),\n",
       "  (212, 1),\n",
       "  (213, 1)],\n",
       " [(82, 1),\n",
       "  (214, 1),\n",
       "  (215, 1),\n",
       "  (216, 1),\n",
       "  (217, 1),\n",
       "  (218, 1),\n",
       "  (219, 1),\n",
       "  (220, 1),\n",
       "  (221, 1),\n",
       "  (222, 1),\n",
       "  (223, 1)],\n",
       " [(3, 1),\n",
       "  (224, 1),\n",
       "  (225, 1),\n",
       "  (226, 1),\n",
       "  (227, 1),\n",
       "  (228, 1),\n",
       "  (229, 1),\n",
       "  (230, 1),\n",
       "  (231, 1),\n",
       "  (232, 1),\n",
       "  (233, 1),\n",
       "  (234, 1)],\n",
       " [(62, 1),\n",
       "  (66, 1),\n",
       "  (72, 1),\n",
       "  (78, 2),\n",
       "  (223, 1),\n",
       "  (235, 1),\n",
       "  (236, 1),\n",
       "  (237, 1),\n",
       "  (238, 1),\n",
       "  (239, 1),\n",
       "  (240, 1),\n",
       "  (241, 1),\n",
       "  (242, 1),\n",
       "  (243, 1),\n",
       "  (244, 1),\n",
       "  (245, 1),\n",
       "  (246, 1),\n",
       "  (247, 1),\n",
       "  (248, 1),\n",
       "  (249, 1),\n",
       "  (250, 1),\n",
       "  (251, 1),\n",
       "  (252, 1),\n",
       "  (253, 1),\n",
       "  (254, 1),\n",
       "  (255, 1),\n",
       "  (256, 1),\n",
       "  (257, 1),\n",
       "  (258, 1),\n",
       "  (259, 1),\n",
       "  (260, 1),\n",
       "  (261, 1),\n",
       "  (262, 1),\n",
       "  (263, 1),\n",
       "  (264, 1),\n",
       "  (265, 1),\n",
       "  (266, 1),\n",
       "  (267, 1),\n",
       "  (268, 1),\n",
       "  (269, 1),\n",
       "  (270, 1),\n",
       "  (271, 1)],\n",
       " [(48, 1),\n",
       "  (58, 1),\n",
       "  (80, 1),\n",
       "  (82, 1),\n",
       "  (102, 1),\n",
       "  (108, 1),\n",
       "  (111, 1),\n",
       "  (155, 1),\n",
       "  (183, 2),\n",
       "  (186, 1),\n",
       "  (199, 1),\n",
       "  (272, 1),\n",
       "  (273, 1),\n",
       "  (274, 1),\n",
       "  (275, 1),\n",
       "  (276, 1),\n",
       "  (277, 1),\n",
       "  (278, 1),\n",
       "  (279, 1),\n",
       "  (280, 1),\n",
       "  (281, 1),\n",
       "  (282, 1),\n",
       "  (283, 1),\n",
       "  (284, 1),\n",
       "  (285, 1),\n",
       "  (286, 1),\n",
       "  (287, 1),\n",
       "  (288, 1),\n",
       "  (289, 1),\n",
       "  (290, 1),\n",
       "  (291, 1),\n",
       "  (292, 1),\n",
       "  (293, 1),\n",
       "  (294, 1),\n",
       "  (295, 1),\n",
       "  (296, 1),\n",
       "  (297, 1),\n",
       "  (298, 1),\n",
       "  (299, 1),\n",
       "  (300, 1),\n",
       "  (301, 1)],\n",
       " [(302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 1)],\n",
       " [(88, 1), (145, 1), (309, 1), (310, 2), (311, 1), (312, 1), (313, 1)],\n",
       " [(200, 2),\n",
       "  (203, 1),\n",
       "  (207, 1),\n",
       "  (219, 1),\n",
       "  (303, 1),\n",
       "  (314, 1),\n",
       "  (315, 1),\n",
       "  (316, 1),\n",
       "  (317, 1),\n",
       "  (318, 1),\n",
       "  (319, 1),\n",
       "  (320, 1),\n",
       "  (321, 1),\n",
       "  (322, 1),\n",
       "  (323, 1),\n",
       "  (324, 1),\n",
       "  (325, 1)],\n",
       " [(32, 3),\n",
       "  (69, 1),\n",
       "  (74, 1),\n",
       "  (79, 1),\n",
       "  (326, 1),\n",
       "  (327, 1),\n",
       "  (328, 1),\n",
       "  (329, 1),\n",
       "  (330, 1),\n",
       "  (331, 1),\n",
       "  (332, 1),\n",
       "  (333, 1),\n",
       "  (334, 1),\n",
       "  (335, 1),\n",
       "  (336, 1),\n",
       "  (337, 1),\n",
       "  (338, 1),\n",
       "  (339, 1),\n",
       "  (340, 1),\n",
       "  (341, 1),\n",
       "  (342, 1),\n",
       "  (343, 1),\n",
       "  (344, 1),\n",
       "  (345, 1),\n",
       "  (346, 1),\n",
       "  (347, 1),\n",
       "  (348, 1),\n",
       "  (349, 1),\n",
       "  (350, 1)],\n",
       " [(71, 1),\n",
       "  (82, 1),\n",
       "  (85, 1),\n",
       "  (116, 1),\n",
       "  (155, 1),\n",
       "  (182, 1),\n",
       "  (183, 1),\n",
       "  (245, 1),\n",
       "  (247, 1),\n",
       "  (351, 1),\n",
       "  (352, 1),\n",
       "  (353, 1),\n",
       "  (354, 1),\n",
       "  (355, 1),\n",
       "  (356, 1),\n",
       "  (357, 1),\n",
       "  (358, 1),\n",
       "  (359, 1),\n",
       "  (360, 1),\n",
       "  (361, 1),\n",
       "  (362, 1),\n",
       "  (363, 1),\n",
       "  (364, 1),\n",
       "  (365, 1),\n",
       "  (366, 1),\n",
       "  (367, 1),\n",
       "  (368, 1)],\n",
       " [(27, 1),\n",
       "  (124, 1),\n",
       "  (271, 1),\n",
       "  (369, 1),\n",
       "  (370, 1),\n",
       "  (371, 1),\n",
       "  (372, 1),\n",
       "  (373, 2),\n",
       "  (374, 1),\n",
       "  (375, 1),\n",
       "  (376, 1),\n",
       "  (377, 1),\n",
       "  (378, 1),\n",
       "  (379, 1),\n",
       "  (380, 1),\n",
       "  (381, 1),\n",
       "  (382, 1),\n",
       "  (383, 1),\n",
       "  (384, 1),\n",
       "  (385, 1),\n",
       "  (386, 1),\n",
       "  (387, 1),\n",
       "  (388, 1),\n",
       "  (389, 1),\n",
       "  (390, 1),\n",
       "  (391, 1),\n",
       "  (392, 1),\n",
       "  (393, 1),\n",
       "  (394, 1)],\n",
       " [(155, 1),\n",
       "  (358, 1),\n",
       "  (395, 1),\n",
       "  (396, 1),\n",
       "  (397, 1),\n",
       "  (398, 1),\n",
       "  (399, 1),\n",
       "  (400, 1)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in sentences]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060*\"家人\" + 0.021*\"逻辑\" + 0.021*\"成功\" + 0.021*\"幸好\" + 0.021*\"为名\"\n"
     ]
    }
   ],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)\n",
    "print(lda.print_topic(3, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025*\"动画\" + 0.018*\"亡灵\" + 0.018*\"皮克斯\" + 0.018*\"动人\" + 0.011*\"纷至沓来\" + 0.011*\"合家欢\" + 0.011*\"流畅\" + 0.011*\"之后\" + 0.011*\"作品\" + 0.011*\"入关\"\n",
      "0.020*\"故事\" + 0.011*\"编剧\" + 0.011*\"一波三折\" + 0.011*\"阴间\" + 0.011*\"充满\" + 0.010*\"伯顿\" + 0.010*\"绚烂\" + 0.010*\"有没有\" + 0.010*\"生老病死\" + 0.010*\"层次分明\"\n",
      "0.021*\"故事\" + 0.012*\"人间\" + 0.012*\"悲欣\" + 0.012*\"交集\" + 0.012*\"俨然\" + 0.012*\"想象力\" + 0.011*\"力荐\" + 0.011*\"新娘\" + 0.011*\"不敢\" + 0.011*\"骷髅\"\n",
      "0.060*\"家人\" + 0.021*\"成功\" + 0.021*\"幸好\" + 0.021*\"逻辑\" + 0.021*\"为名\" + 0.021*\"区别\" + 0.021*\"以爱\" + 0.021*\"家长\" + 0.021*\"人生\" + 0.021*\"梦想\"\n",
      "0.039*\"一边\" + 0.039*\"情节\" + 0.020*\"意义\" + 0.020*\"一个\" + 0.020*\"故事\" + 0.020*\"探讨\" + 0.020*\"推荐\" + 0.020*\"指导\" + 0.020*\"皮克斯\" + 0.020*\"画面\"\n",
      "0.039*\"任性\" + 0.021*\"小孩\" + 0.021*\"害怕\" + 0.021*\"情绪\" + 0.021*\"女人\" + 0.021*\"原因\" + 0.021*\"一直\" + 0.021*\"讲些\" + 0.021*\"爱恨\" + 0.021*\"惊喜\"\n",
      "0.029*\"元素\" + 0.018*\"墨西哥\" + 0.018*\"煽情\" + 0.018*\"代价\" + 0.018*\"巧妙\" + 0.018*\"偶像\" + 0.018*\"成长\" + 0.018*\"依旧\" + 0.018*\"招架\" + 0.018*\"实在\"\n",
      "0.062*\"电影院\" + 0.062*\"唏嘘\" + 0.032*\"皮克斯\" + 0.032*\"孩子\" + 0.032*\"哭成\" + 0.032*\"一起\" + 0.032*\"大人\" + 0.032*\"可恶\" + 0.032*\"左右\" + 0.032*\"一次\"\n",
      "0.002*\"迷幻\" + 0.002*\"阴阳交错\" + 0.002*\"老少咸宜\" + 0.002*\"脑袋\" + 0.002*\"蒂姆\" + 0.002*\"绚烂\" + 0.002*\"世界观\" + 0.002*\"4.5\" + 0.002*\"生老病死\" + 0.002*\"骷髅\"\n",
      "0.017*\"生命\" + 0.017*\"四星\" + 0.017*\"vs\" + 0.017*\"墨西哥\" + 0.017*\"这部\" + 0.017*\"善莫大焉\" + 0.017*\"大有\" + 0.016*\"故事\" + 0.016*\"无法\" + 0.016*\"莽撞\"\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(num_topics=10,num_words=10):\n",
    "    print(topic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将词语分类并记录其位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# 1. 文本切割\n",
    "# \"\"\"\n",
    "\n",
    "def sent2word(sentence):\n",
    "    segList = jieba.cut(sentence)\n",
    "    segResult = []\n",
    "    for w in segList:\n",
    "        segResult.append(w)\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        stopwords = [w.strip() for w in f.readlines()]\n",
    "\n",
    "    newSent = []\n",
    "    for word in segResult:\n",
    "        if word in stopwords:\n",
    "            # print \"stopword: %s\" % word\n",
    "            continue\n",
    "        else:\n",
    "            newSent.append(word)\n",
    "    return newSent\n",
    "wordDict = sent2word(npl_df[(npl_df['title']=='寻梦环游记')][:20].comment.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# 2. 情感定位\n",
    "# \"\"\"\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def classifyWords(wordDict):\n",
    "    # (1) 情感词\n",
    "    with open('BosonNLP_sentiment_score.txt', 'r') as f:\n",
    "        senList = [w.strip() for w in f.readlines()]\n",
    "\n",
    "    senDict = {}\n",
    "    for s in senList:\n",
    "        try:\n",
    "            senDict[s.split(' ')[0]] = s.split(' ')[1]\n",
    "        except:\n",
    "            continue\n",
    "    # (2) 否定词\n",
    "    with open('notDict.txt', 'r') as f:\n",
    "        notList = [w.strip() for w in f.readlines()]\n",
    "    # (3) 程度副词\n",
    "    with open('degreeDict.txt', 'r') as f:\n",
    "        degreeList = [w.strip() for w in f.readlines()]\n",
    "    degreeDict = {}\n",
    "    for s in degreeList:\n",
    "        try:\n",
    "            degreeDict[s.split(',')[0]] = s.split(',')[1]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    senWord = {}\n",
    "    notWord = {}\n",
    "    degreeWord = {}\n",
    "    \n",
    "    for word in wordDict:\n",
    "        if word in senDict.keys() and word not in notList and word not in degreeDict.keys():\n",
    "            senWord[word] = senDict[word]\n",
    "        elif word in notList and word not in degreeDict.keys():\n",
    "            notWord[word] = -1\n",
    "        elif word in degreeDict.keys():\n",
    "            degreeWord[word] = degreeDict[word]\n",
    "    return senWord, notWord, degreeWord\n",
    "senWord, notWord, degreeWord = classifyWords(wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.522944806467"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# 3. 情感聚合\n",
    "# \"\"\"\n",
    "def scoreSent(senWord, notWord, degreeWord, segResult):\n",
    "    W = 1\n",
    "    score = 0\n",
    "    # 存所有情感词的位置的列表\n",
    "    senLoc = 0\n",
    "    notLoc = 0\n",
    "    degreeLoc = 0\n",
    "    #senloc = -1\n",
    "    # notloc = -1\n",
    "    # degreeloc = -1\n",
    "    \n",
    "    # 遍历句中所有单词segResult，i为单词绝对位置\n",
    "    for i in range(0, len(segResult)):\n",
    "        # 如果该词为情感词\n",
    "        if segResult[i] in senWord.keys():\n",
    "            # 直接添加该情感词分数\n",
    "            score += W * float(senWord[segResult[i]])\n",
    "            \n",
    "            # loc为情感词位置列表的序号\n",
    "            senLoc += 1\n",
    "            #print(\"score = %f\" % score)\n",
    "#             if senloc < len(senLoc) - 1:\n",
    "#                 # 判断该情感词与下一情感词之间是否有否定词或程度副词\n",
    "#                 # j为绝对位置\n",
    "#                 for j in range(senLoc[senloc], senLoc[senloc + 1]):\n",
    "#                     # 如果有否定词\n",
    "#                     if j in notLoc:\n",
    "#                         W *= -1\n",
    "#                     # 如果有程度副词\n",
    "#                     elif j in degreeLoc:\n",
    "#                         W *= float(degreeWord[j])\n",
    "#         # i定位至下一个情感词\n",
    "#         if senloc < len(senLoc) - 1:\n",
    "#             i = senLoc[senloc + 1]\n",
    "    return score\n",
    "\n",
    "scoreSent(senWord, notWord, degreeWord, wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>title</th>\n",
       "      <th>rating.value_y</th>\n",
       "      <th>rating.value_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4571</th>\n",
       "      <td>广州试片会。主持人在现场透露：Coco原本涉及到亡灵题材，是不能引进的。但是在过审时，当场看...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>供奉的遗像是牵引家人回家的通道，驻留的记忆是保持亡灵存续的神力，热闹的音乐是唤醒思念启封的药...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>不得不服皮克斯，一边和你探讨梦想、成功、家庭的意义；一边用各种充满想象力的情节、画面、音乐，...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>对于皮克斯，是很垃圾的一部电影，乏善可陈，没有一点儿新意，看了开头就知道结尾，完全是浪费时间...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>Up的一体两面，成长的代价与偶像的崩塌，但墨西哥元素揉的实在太巧妙，煽情起来也依旧让我难以招架。</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>上映的第四个星期才看，期待很高是真，不如预期也是真。大概是皮克斯正常偏低水准，抓住了“亲情”...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>可恶的皮克斯，又一次在电影院哭成傻比</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>11/16@迪斯尼上海 四星半。谢邀试片会，Disney x Pixar这部新作在故事磨合与...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>提前宣布这部是2018年奥斯卡最佳动画长片，感恩节之后看简直哭到颤抖，明明是被玩滥的家庭取舍...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4580</th>\n",
       "      <td>请皮克斯收下我的膝盖……看片的时候大家在讨论基佬没有后代，是不是在阴间也会消失得很快。ಥ_ಥ</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>“Family comes first.”的前提不应该是互相忍让，而是相互理解。（PS.亡灵...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4583</th>\n",
       "      <td>哇，太棒了，阴阳交错，仙境漫游，充满童心的想象力，彩色温暖迷幻，故事一波三折，绚烂的阴间图景...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4584</th>\n",
       "      <td>4.5 在结局猛烈的组合拳煽情下哭到眼睛都睁不开...虽然还是有不少皮克斯惯性的程式，依旧是...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4585</th>\n",
       "      <td>死后世界还是这么三六九等，穷逼死了还是穷逼，那还不如被快点遗忘消失掉啊！</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>电影院左右的大人带着孩子一起哭得唏嘘唏嘘的。只是完全不是我的哭点。</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>人的一生要经历两次死亡：第一次是肉身的死亡，第二次则是从爱你的人的记忆里消失。当这个生者的世...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>因为是家人，就有权干涉你的人生、你的梦想，理由是“一家人就要整整齐齐”。片中家人所做的事，跟...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>在中段那个可以比肩皮克斯之前作品任何动人瞬间的静默时刻之后，一连串落入窠臼的情节却纷至沓来，...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>虽然很不错，但总觉得少了点什么，细想一下原因可能有三：1，祖祖祖奶奶的爱恨表达很草率，唯一的...</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>很久没有见过大家纷纷摘下眼镜掏纸巾的动人时刻了</td>\n",
       "      <td>寻梦环游记</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  title  \\\n",
       "4571  广州试片会。主持人在现场透露：Coco原本涉及到亡灵题材，是不能引进的。但是在过审时，当场看...  寻梦环游记   \n",
       "4572  供奉的遗像是牵引家人回家的通道，驻留的记忆是保持亡灵存续的神力，热闹的音乐是唤醒思念启封的药...  寻梦环游记   \n",
       "4573  不得不服皮克斯，一边和你探讨梦想、成功、家庭的意义；一边用各种充满想象力的情节、画面、音乐，...  寻梦环游记   \n",
       "4574  对于皮克斯，是很垃圾的一部电影，乏善可陈，没有一点儿新意，看了开头就知道结尾，完全是浪费时间...  寻梦环游记   \n",
       "4575   Up的一体两面，成长的代价与偶像的崩塌，但墨西哥元素揉的实在太巧妙，煽情起来也依旧让我难以招架。  寻梦环游记   \n",
       "4576  上映的第四个星期才看，期待很高是真，不如预期也是真。大概是皮克斯正常偏低水准，抓住了“亲情”...  寻梦环游记   \n",
       "4577                                 可恶的皮克斯，又一次在电影院哭成傻比  寻梦环游记   \n",
       "4578  11/16@迪斯尼上海 四星半。谢邀试片会，Disney x Pixar这部新作在故事磨合与...  寻梦环游记   \n",
       "4579  提前宣布这部是2018年奥斯卡最佳动画长片，感恩节之后看简直哭到颤抖，明明是被玩滥的家庭取舍...  寻梦环游记   \n",
       "4580     请皮克斯收下我的膝盖……看片的时候大家在讨论基佬没有后代，是不是在阴间也会消失得很快。ಥ_ಥ  寻梦环游记   \n",
       "4582  “Family comes first.”的前提不应该是互相忍让，而是相互理解。（PS.亡灵...  寻梦环游记   \n",
       "4583  哇，太棒了，阴阳交错，仙境漫游，充满童心的想象力，彩色温暖迷幻，故事一波三折，绚烂的阴间图景...  寻梦环游记   \n",
       "4584  4.5 在结局猛烈的组合拳煽情下哭到眼睛都睁不开...虽然还是有不少皮克斯惯性的程式，依旧是...  寻梦环游记   \n",
       "4585               死后世界还是这么三六九等，穷逼死了还是穷逼，那还不如被快点遗忘消失掉啊！  寻梦环游记   \n",
       "4586                  电影院左右的大人带着孩子一起哭得唏嘘唏嘘的。只是完全不是我的哭点。  寻梦环游记   \n",
       "4587  人的一生要经历两次死亡：第一次是肉身的死亡，第二次则是从爱你的人的记忆里消失。当这个生者的世...  寻梦环游记   \n",
       "4588  因为是家人，就有权干涉你的人生、你的梦想，理由是“一家人就要整整齐齐”。片中家人所做的事，跟...  寻梦环游记   \n",
       "4589  在中段那个可以比肩皮克斯之前作品任何动人瞬间的静默时刻之后，一连串落入窠臼的情节却纷至沓来，...  寻梦环游记   \n",
       "4590  虽然很不错，但总觉得少了点什么，细想一下原因可能有三：1，祖祖祖奶奶的爱恨表达很草率，唯一的...  寻梦环游记   \n",
       "4591                            很久没有见过大家纷纷摘下眼镜掏纸巾的动人时刻了  寻梦环游记   \n",
       "\n",
       "      rating.value_y  rating.value_x  \n",
       "4571             9.0             5.0  \n",
       "4572             9.0             5.0  \n",
       "4573             9.0             5.0  \n",
       "4574             9.0             2.0  \n",
       "4575             9.0             5.0  \n",
       "4576             9.0             3.0  \n",
       "4577             9.0             5.0  \n",
       "4578             9.0             4.0  \n",
       "4579             9.0             5.0  \n",
       "4580             9.0             5.0  \n",
       "4582             9.0             5.0  \n",
       "4583             9.0             5.0  \n",
       "4584             9.0             5.0  \n",
       "4585             9.0             4.0  \n",
       "4586             9.0             3.0  \n",
       "4587             9.0             4.0  \n",
       "4588             9.0             3.0  \n",
       "4589             9.0             3.0  \n",
       "4590             9.0             4.0  \n",
       "4591             9.0             4.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npl_df[npl_df['title']=='寻梦环游记'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40496507,  1.07877657,  0.75825657, -0.88958346, -0.10847657,\n",
       "        0.79416069, -1.09773176,  1.17644573,  1.83800763, -0.75267598,\n",
       "       -0.28802841,  1.82640337,  0.85600428, -1.4680098 , -1.09590307,\n",
       "       -0.8694442 , -0.02236642,  0.47194671, -0.99598886, -0.80682794])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "sent2word(npl_df[npl_df['title']=='寻梦环游记'].comment.values[1])\n",
    "def sentence_to_score(x):\n",
    "    wordDict = sent2word(x)\n",
    "    senWord, notWord, degreeWord = classifyWords(wordDict)\n",
    "    return scoreSent(senWord, notWord, degreeWord, wordDict)\n",
    "scale(npl_df[npl_df['title']=='寻梦环游记'][:20]['comment'].apply(lambda x:sentence_to_score(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这种分数相差很大。。。。感觉用来训练，准确率貌似会很低。。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 换一种思路，使用Word2Vec构建词义向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unlike    52249\n",
       "like      41176\n",
       "Name: rating_classify, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_rating(value):\n",
    "    if value<=3:\n",
    "        return 'unlike'\n",
    "    elif value >= 4:\n",
    "        return 'like'\n",
    "    else:\n",
    "        return 'no_rating'\n",
    "npl_df['rating_classify'] = npl_df['rating.value_x'].apply(classify_rating)\n",
    "npl_df['rating_classify'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>title</th>\n",
       "      <th>rating.value_y</th>\n",
       "      <th>rating.value_x</th>\n",
       "      <th>rating_classify</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72600</th>\n",
       "      <td>[BBC, 英伦, 范儿, 很足, 女一, 孔雀蓝, 长裙, 真, 精致, 俄国, 名著, ...</td>\n",
       "      <td>麦克白夫人</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>unlike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81247</th>\n",
       "      <td>[回望, 人生, 一切都是, 错, 刚刚, 好, 接受, 平庸, 相信, 伟大, 重要]</td>\n",
       "      <td>你好布拉德</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>unlike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32289</th>\n",
       "      <td>[不忍, 直视, 阴阳, 路, 二十年, 情怀, 蔡卓妍, 演技, 尴尬, 癌, 要犯]</td>\n",
       "      <td>常在你左右</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>unlike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50962</th>\n",
       "      <td>[12, 岁, 高智商, 熊, 孩子, 恶少, 完成, 变态, 绑架, 杀人案]</td>\n",
       "      <td>安全邻域</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>unlike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29688</th>\n",
       "      <td>[When,  , I,  , think,  , about,  , that,  , l...</td>\n",
       "      <td>老幸运</td>\n",
       "      <td>7.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  title  \\\n",
       "72600  [BBC, 英伦, 范儿, 很足, 女一, 孔雀蓝, 长裙, 真, 精致, 俄国, 名著, ...  麦克白夫人   \n",
       "81247       [回望, 人生, 一切都是, 错, 刚刚, 好, 接受, 平庸, 相信, 伟大, 重要]  你好布拉德   \n",
       "32289       [不忍, 直视, 阴阳, 路, 二十年, 情怀, 蔡卓妍, 演技, 尴尬, 癌, 要犯]  常在你左右   \n",
       "50962           [12, 岁, 高智商, 熊, 孩子, 恶少, 完成, 变态, 绑架, 杀人案]   安全邻域   \n",
       "29688  [When,  , I,  , think,  , about,  , that,  , l...    老幸运   \n",
       "\n",
       "       rating.value_y  rating.value_x rating_classify  \n",
       "72600             6.5             2.0          unlike  \n",
       "81247             7.5             3.0          unlike  \n",
       "32289             5.1             2.0          unlike  \n",
       "50962             6.4             3.0          unlike  \n",
       "29688             7.5             4.0            like  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#先去掉停用词\n",
    "def clean_stop_words(comment):\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        stopwords = set([w.strip() for w in f.readlines()])\n",
    "    comments=jieba.lcut(comment)\n",
    "    for word in stopwords:\n",
    "        while word in comments:\n",
    "            comments.remove(word)\n",
    "    return comments\n",
    "npl_df.loc[:,'comment'] = npl_df.loc[:,'comment'].apply(clean_stop_words)\n",
    "npl_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了避免分类学习器偏向的问题，对三类样本做1:1:1采样，组成新的样本。<br>\n",
    "现在改成2分问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unlike    41176\n",
       "like      41176\n",
       "Name: rating_classify, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df = pd.concat([npl_df[npl_df['rating_classify'] == 'like']\n",
    "                   ,npl_df[npl_df['rating_classify'] == 'unlike'].sample(41176)])\n",
    "\n",
    "vec_df['rating_classify'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import scale\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练集 测试集的切分\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    X = vec_df['comment'].values\n",
    "    y = vec_df['rating_classify'].values\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,random_state=2018) #random_state随机种子\n",
    "    \n",
    "    np.save('y_train.npy',y_train)\n",
    "    np.save('y_test.npy',y_test)\n",
    "    \n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每一句话词计算并取平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_sentence_vector(text, size, imdb_w2v):\n",
    "    vec = np.zeros(size).reshape((1,size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1,size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /=count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_vecs(x_train, x_test):\n",
    "    n_dim = 100\n",
    "    imdb_w2v = Word2Vec(size=n_dim, min_count=3)\n",
    "    imdb_w2v.build_vocab(x_train)\n",
    "    imdb_w2v.train(x_train,total_examples=len(x_train), epochs=10)\n",
    "    train_vecs = np.concatenate([build_sentence_vector(z, n_dim, imdb_w2v) for z in x_train])\n",
    "    train_vecs = scale(train_vecs)\n",
    "    np.save('train_vecs.npy', train_vecs)\n",
    "    print(train_vecs.shape)\n",
    "    imdb_w2v.train(x_test,total_examples=len(x_train), epochs=10)\n",
    "    imdb_w2v.save('model.pkl')\n",
    "    test_vecs = np.concatenate([build_sentence_vector(z, n_dim, imdb_w2v) for z in x_test])\n",
    "    test_vecs = scale(test_vecs)\n",
    "    np.save('test_vecs.npy', test_vecs)\n",
    "    print(test_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取 训练集、测试集、分类标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train_vec = np.load('train_vecs.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    test_vecs = np.load('test_vecs.npy')\n",
    "    y_test = np.load('y_test.npy')\n",
    "    return train_vec,y_train,test_vecs,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_train(train_vecs, y_train, test_vecs, y_test):\n",
    "    clf = SVC(kernel = 'linear', verbose = False) #'rbf'\n",
    "    clf.fit(train_vecs, y_train)\n",
    "    joblib.dump(clf, 'svm_model_1.pkl')\n",
    "    print(clf.score(test_vecs,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predict_vecs(words):\n",
    "    n_dim = 100\n",
    "    imdb_w2v = Word2Vec.load('model.pkl')\n",
    "    train_vecs = build_sentence_vector(words, n_dim, imdb_w2v)\n",
    "    return train_vecs\n",
    "def predict(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words_vec = get_predict_vecs(words)\n",
    "    clf=joblib.load('svm_model_1.pkl')\n",
    "    result=clf.predict(words_vec)\n",
    "    print('%s......  --> svm:  %s'%(s[:10],result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('loading Data......')\n",
    "    x_train, x_test = load_data()\n",
    "    get_train_vecs(x_train,x_test)\n",
    "    print('Model training......')\n",
    "    train_vecs,y_train,test_vecs,y_test = get_data()\n",
    "#     svm_train(train_vecs,y_train,test_vecs,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "train()\n",
    "endTime = datetime.datetime.now()\n",
    "print(endTime-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练结果有点儿惨：[LibSVM]0.413627639155。**\n",
    "* 做了向量化后，加入一个scale试试。训练了n久，结果：0.43769993602,还是惨\n",
    "* 只有提高把向量化参数 n_dim = 30，改 n_dim =100了，结果训练了1:20:14.150902，然后提高了一点点0.469449776072\n",
    "* 有用xgb试了试，也很惨，训练时间5秒钟，correct=0.334693\n",
    "\n",
    "#### 感觉导致结果比较差的原因是不是因为之前数据处理的时候就有问题。回过去重新检查一下数据\n",
    "* 提前做了一个停用词的处理，将df中所有的comments处理了停用词，结果：训练了0:39:22.197936，然后降低了一点点0.455534229047\n",
    "\n",
    "#### 经过寒神的指教，准备把三分问题变为2分问题\n",
    "* 小于等于3星 dislike, 大于3星 like\n",
    "* 然后xgb就运作的很好了，svm一直没有跑成功，不知道为何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>title</th>\n",
       "      <th>rating.value_y</th>\n",
       "      <th>rating.value_x</th>\n",
       "      <th>rating_classify</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[笑, 太, 疯癫, 笑, 爱, 烂, 片, 影史, 最, 经典, 烂片, 诞生, 迷影, ...</td>\n",
       "      <td>灾难艺术家</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  title  rating.value_y  \\\n",
       "0  [笑, 太, 疯癫, 笑, 爱, 烂, 片, 影史, 最, 经典, 烂片, 诞生, 迷影, ...  灾难艺术家             7.8   \n",
       "\n",
       "   rating.value_x rating_classify  \n",
       "0             5.0            like  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用xgb测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0426923440674\n",
      "{'colsample_bylevel': 0.65, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 20, 'subsample': 0.65}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"  #并行训练\n",
    "rng = np.random.RandomState(2017)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "X = vec_df['rating.value_y'].values\n",
    "y = vec_df['rating_classify'].values\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,random_state=2018) #random_state随机种子\n",
    "\n",
    "\n",
    "train_vecs,y_train,test_vecs,y_test = get_data()\n",
    "\n",
    "def get_y_int(y):\n",
    "    for i in range(0,len(y)):\n",
    "        if y[i] == 'unlike':\n",
    "            y[i] = 0\n",
    "        elif y[i] == 'like':\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 999\n",
    "    return y\n",
    "\n",
    "y_train_int = get_y_int(y_train)\n",
    "y_test_int = get_y_int(y_test)\n",
    "\n",
    "# X = pd.DataFrame(train_vecs)\n",
    "# X['score_now'] = X_train\n",
    "# X = X[[c for c in X.columns]].values\n",
    "\n",
    "X = train_vecs\n",
    "y = y_train_int\n",
    "\n",
    "\n",
    "\n",
    "# param_grid = {\n",
    "#               'max_depth': [3, 4, 5],\n",
    "#               'n_estimators': [20, 40],   #, 50, 60, 80, 100, 200, 400\n",
    "#               'learning_rate': [0.01, 0.02],  #, 0.05, 0.1, 0.2\n",
    "#               'subsample': [0.65, 0.7, 0.8],\n",
    "#               'colsample_bylevel':[0.65] #, 0.7, 0.8\n",
    "#              }\n",
    "\n",
    "param_grid = {\n",
    "              'max_depth': [3],\n",
    "              'n_estimators': [20],\n",
    "              'learning_rate': [0.01, 0.02],\n",
    "              'subsample': [0.65],\n",
    "              'colsample_bylevel':[0.65]\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=2017)\n",
    "rgs = GridSearchCV(xgb_model, param_grid, n_jobs=8, cv=5)\n",
    "rgs.fit(X, y)\n",
    "print(rgs.best_score_)\n",
    "print(rgs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0426923440674<br>\n",
    "{'colsample_bylevel': 0.65, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 20, 'subsample': 0.65}<br>\n",
    "0.0866109239447<br>\n",
    "{'colsample_bylevel': 0.65, 'learning_rate': 0.02, 'max_depth': 5, 'n_estimators': 40, 'subsample': 0.65}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_model_1.pkl']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rgs, 'xgb_model_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_predict(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words_vec = get_predict_vecs(words)\n",
    "    clf=joblib.load('xgb_model_2.pkl')\n",
    "    result=clf.predict(words_vec)\n",
    "    if result > 0.5:\n",
    "        print('%s......  --> xgb:  喜爱'%(s[:10]))\n",
    "    else:\n",
    "        print('%s......  --> xgb:  不喜爱'%(s[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "供奉的遗像是牵引家人......  --> xgb:  喜爱\n",
      "供奉的遗像是牵引家人......  --> svm:  ['unlike']\n"
     ]
    }
   ],
   "source": [
    "s = '供奉的遗像是牵引家人回家的通道，驻留的记忆是保持亡灵存续的神力，热闹的音乐是唤醒思念启封的药引。我为你写了首歌，穿越浩瀚的岁月烟尘，捱过冰冷的孤独冬季，横跨漫长的天人之路，在你老去的时候，唱给你听。这瑰丽的灯火万家，摇曳的烛光千盏，不如你梳着麻花辫坐在床头时，眼里闪烁的星光璀璨。'\n",
    "xgb_predict(s)\n",
    "predict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无后同性恋们的一场极......  --> xgb:  不喜爱\n",
      "无后同性恋们的一场极......  --> svm:  ['normal']\n"
     ]
    }
   ],
   "source": [
    "s = '无后同性恋们的一场极大的噩梦。'\n",
    "xgb_predict(s)\n",
    "predict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "漫天好评太夸张了吧。......  --> xgb:  不喜爱\n",
      "漫天好评太夸张了吧。......  --> svm:  ['unlike']\n"
     ]
    }
   ],
   "source": [
    "s = '漫天好评太夸张了吧。。。Pixar的叙事与价值观内核真是愈发俗套保守了，这还真是个全世界都要回归家庭本位的年代咩。。。'\n",
    "xgb_predict(s)\n",
    "predict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最后胡巴跟笨笨跳舞的......  --> xgb:  不喜爱\n",
      "最后胡巴跟笨笨跳舞的......  --> svm:  ['normal']\n"
     ]
    }
   ],
   "source": [
    "s = r'最后胡巴跟笨笨跳舞的彩蛋竟然是全场最佳，不断脑补是我GAI跟花园宝宝在唱歌……除此以外剧情大概适合三至五岁小宝宝。'\n",
    "xgb_predict(s)\n",
    "predict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过年看个热闹吧，比第......  --> xgb:  不喜爱\n",
      "过年看个热闹吧，比第......  --> svm:  ['like']\n"
     ]
    }
   ],
   "source": [
    "s = r'过年看个热闹吧，比第一部差劲'\n",
    "xgb_predict(s)\n",
    "predict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过年看个热闹吧，比第......  --> xgb:  不喜爱\n",
      "过年看个热闹吧，比第......  --> svm:  ['normal']\n"
     ]
    }
   ],
   "source": [
    "comment = '名著被毁不是第一次。除了交心的《女儿情》，美妆造型都是辣眼睛，梁咏琪国师扮相看到自己也想哭吧？赵丽颖至少演得比冯绍峰好一点。除了特效做得精致，其他东西都牺牲了。奇幻、爱情、搞笑，尽力达到春节档各种元素。重工业大片剧本也不能轻易落下，为了资本投降可违，毕竟好故事总是能打动人的。'\n",
    "xgb_predict(comment)\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过年看个热闹吧，比第......  --> xgb:  喜爱\n",
      "过年看个热闹吧，比第......  --> svm:  ['like']\n"
     ]
    }
   ],
   "source": [
    "comment = '看了点映，女儿国这部真正适合到大屏幕上观看，3D视觉效果极为震撼，超过前作《三打》很多，赵丽颖大银幕上太美了，我要是唐僧一样走不动了。'\n",
    "xgb_predict(comment)\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过年看个热闹吧，比第......  --> xgb:  喜爱\n",
      "过年看个热闹吧，比第......  --> svm:  ['unlike']\n"
     ]
    }
   ],
   "source": [
    "comment = '那一瞬你从天而降，这一次我奋身而跃，是心跳的重叠，亦是天命的交错。初遇泪目，别离无奈，再特立独行的爱情也敌不过所谓的使命，什么王权富贵戒律清规，什么爱一人爱众生，什么若离女儿国万物凋零，俱是佛祖的棋子。世俗毁掉你的真心，还要把它解作执念。苦海无涯，命运结界；执子之袖，若有来生。'\n",
    "xgb_predict(comment)\n",
    "predict(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### xgb比svm要准确得多"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
